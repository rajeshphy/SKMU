---
layout: post
title: "Talk on AI"
date: 2026-01-26 08:00:00 +0530
categories: lecture
tags: Research 
permalink: /AI-Talk/
---

In 1950, Alan Turing published a seminal paper titled "Computing Machinery and Intelligence," in which he asked the provocative question: "Can machines think?" This question laid the foundation for the field of artificial intelligence (AI) and has since sparked decades of research and debate.

It is difficult to formally define "**thinking**," as it will be used as a hard criterion to evaluate whether an AI is thinking or not. However, Turing proposed a practical test, imitation game, now known as the **Turing Test**.

The Turing Test involves a human evaluator who engages in natural language conversations with both a human and a machine, without knowing which is which. If the evaluator cannot reliably distinguish between the human and the machine based on their responses, the machine is said to have passed the Turing Test, demonstrating a form of intelligence.

However, passing the Turing Test does not necessarily imply that a machine possesses true understanding or consciousness. The outcome of the test can be influenced by various factors:

- How long the conversation lasts
- The topics discussed
- The skill of the human participant
- The sophistication of the machine's responses

1956 marked a significant milestone in the history of AI with the Dartmouth Conference in Hanover, New Hampshire, USA, where the term "artificial intelligence" was coined. This conference brought together researchers from various fields, including computer science, psychology, and mathematics, to explore the possibilities of creating intelligent machines.

---

## Understanding the terminology

There is an array of terms associated with AI that are often used interchangeably but have distinct meanings:
- Expert Systems: This example is seen commonly in Telecom and Medical Diagnosis systems. They use **IVR** (Interactive Voice Response) to interact with users and provide expert-level responses based on a predefined set of rules and knowledge bases. When you call a customer support line and navigate through a series of prompts: "Press 1 for english, Press 2 for hindi..." you are interacting with an expert system. 
>> Notice here that the system does not "learn" from your inputs; it simply follows a set of programmed rules to guide you to the right information or service.

- **Machine Learning (ML):** It is a more sophisticated form of AI that enables systems to learn and improve from experience without being explicitly programmed. Example includes your telecom service provider analyzing your call numbers to classify them as important or spam, and accordingly prioritizing or blocking them.

> A digital photo of a rose is not seen as a “flower” by a computer—it is seen as millions of tiny squares (pixels), and each pixel has numerical values representing color (red, green, blue). One pixel being red does not automatically tell the computer that the next pixel will also be red. To a machine, every pixel is just a number.
> The algorithm looks for patterns of pixels, not individual pixels. By analyzing large datasets of images labeled as "flowers" and "not flowers," the machine learning model can learn to recognize patterns that distinguish flowers from non-flowers, enabling it to classify new images accurately. Over many images, it learns features like edges, curves, petal shapes, and color gradients.

>> This is the first point where humans stop writing rules and start giving data.

- Neural Networks: These are specialized ML algorithms inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) that process and transmit information.

>  A human neuron receives input signals from other neurons through its dendrites, passes the signals through axons, and transmits them across synapses to other neurons, enabling communication and information processing in the nervous system.

> The rose image example can be extended to neural networks. A neural network processes the pixel data through multiple layers of interconnected nodes, where each layer extracts increasingly complex features from the image. The first layer might detect simple edges, the next layer could identify shapes like petals, and subsequent layers would combine these features to recognize the overall structure of a flower. Through training on large datasets, the neural network learns to associate specific patterns of pixel values with the concept of a "flower," allowing it to classify new images accurately.

- Deep Learning: This is another subset of ML that utilizes multi-layered neural networks to analyze complex patterns in large datasets. Deep learning has been particularly successful in tasks such as image and speech recognition.
 

> Continuing with the rose image example, deep learning involves using neural networks with many layers (hence "deep") to process the pixel data. Each layer of the network learns to extract more abstract and complex features from the image. For instance, the initial layers might focus on basic shapes and colors, while deeper layers could identify intricate details like petal arrangements and textures. By training on vast amounts of labeled images, deep learning models can achieve high accuracy in recognizing flowers, even in varied conditions such as different lighting or angles.

>> This jump—from thousands to millions of parameters—is why AI suddenly exploded after 2015.

- Generative AI: This refers to AI systems capable of generating new content, such as text, images, or music, based on learned patterns from existing data. Examples include language models like GPT-3 and image generation models like DALL-E. These are specilized deep learning models designed to create new content that resembles human-generated data.

> “If I ask a calculator: ‘Write me a poem,’ it fails completely.
> If I ask ChatGPT to write a poem in the style of Rabindranath Tagore about the beauty of nature, it can generate a poem that captures the essence of Tagore's style and themes, demonstrating its ability to create human-like text based on learned patterns.”


- AGI (Artificial General Intelligence): This represents a theoretical form of AI that possesses human-like cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. AGI remains a goal for future AI research and development. Many experts believe that it will take several decades to achieve AGI, if it is achievable at all. But the the CEO of OpenAI, Sam Altman, has predicted that AGI could be developed as early as 2028.
> Other researchers argue AGI may never arrive, because intelligence is embodied, emotional, and social — not just linguistic.


---

## The Turning Point in the AI Revolution

Two major developments in the last decade fundamentally changed the direction and speed of artificial intelligence. First, in **2015**, Google released **TensorFlow** as an open-source deep learning framework. This made advanced AI tools freely available to researchers, startups, and universities worldwide, dramatically lowering the entry barrier. The second decisive moment came when **OpenAI**, co-founded by *Elon Musk* and led by *Sam Altman*, released **large language models (LLMs)** to the public, most notably **ChatGPT** on Nov 30, 2022. For the first time, highly capable AI systems became directly accessible to ordinary users, not just specialists.

The speed of adoption of generative AI has been unprecedented. For perspective, **Netflix took about 3.5 years to reach one million users**. In contrast, **ChatGPT reached one million users in just five days** after launch. Within **two months**, it crossed **100 million users**, making it one of the fastest-growing consumer applications in history. Today, its user base is around **800 millions**, signaling not just popularity but a structural shift in how people interact with technology.

AI is now beginning to influence **politics, law, and global power structures**.

> **News snapshot (Legal incident):**  
> In the United States, several lawyers were **sanctioned by courts for submitting legal briefs containing fake case citations generated by AI tools**. In one widely reported case, attorneys relied on an AI system for legal research and filed documents that cited **non-existent court cases and fabricated judgments**. The sanctions were imposed because the lawyers **failed to verify the AI-generated content**, violating their professional duty to ensure accuracy before submitting documents to the court.

> **What caused the sanctions:**  
> Courts ruled that while AI can assist in research, **responsibility lies entirely with the human lawyer**. The use of AI was not the problem; the problem was **blind trust in AI outputs**, leading to misinformation in official legal proceedings.

> **Broader implication:**  
> These incidents triggered judicial warnings, policy discussions, and renewed emphasis on **AI safety, regulation, and training**, particularly in the U.S., where AI capability is now viewed as a matter of **national competence and global leadership**.

---

##  Key Aspects of Modern AI: Data, Scale, Environment, and Ethics


#### Data and Model Scale

> **AI Training at an Incredible Scale:**  
> To understand the scale of modern AI, consider this recent example of a chatbot. Its neural network has **1.8 trillion weights** (for comparison, the human brain has around 100 trillion). It was trained on **13 trillion words** collected from the Internet. The training cost around **$63 million** and took about **five to six months** on powerful supercomputers.

> **In Simple Terms:**  
> Imagine teaching a child to read and write using every book, article, and website in the world. Now imagine doing that **millions of times faster than a human**—that’s roughly what these AI systems are doing, using enormous amounts of data, money, and energy.

> **Visualizing the Data:**  
> If the 13 trillion words were printed in books of 100,000 words each, and these books were placed on shelves half a metre wide (100 books per shelf), the bookshelves would form a line **650 kilometres long**—almost the distance from **Dhamri to Ranchi**.

> **Visualizing the Model:**  
> Printing all **1.8 trillion weights** on paper would cover **30,000 football fields**. Placed side by side, these fields could stretch all the way from **India to Iran**.  

> **If Using a Regular Laptop:**  
> Training this AI on a normal consumer laptop would take about **7 million years**, showing why **distributed supercomputers** are essential for modern AI development.







#### Moore’s Law and AI

> **What is Moore’s Law?**  
> Moore’s Law is the observation that the number of **transistors on a computer chip roughly doubles every two years**, making computers faster and more powerful over time. This exponential growth in computing power has been a key driver of AI progress.

> **Impact on AI:**  
> AI, especially **deep learning and large language models**, requires huge amounts of computation. Moore’s Law means that **faster processors and more powerful GPUs** allow researchers to train bigger models, process more data, and achieve better performance.

> **Layman Example:**  
> Imagine trying to solve a puzzle with millions of pieces. A slow computer (older chips) would take years, but thanks to **Moore’s Law**, modern chips let AI solve puzzles **much faster**. This has made breakthroughs like **ChatGPT, self-driving cars, and advanced image recognition** possible in just the last few years.

> **Limitation:**  
> Moore’s Law is slowing down, which means future AI might need **more energy-efficient hardware, specialized chips, or innovative computing methods** to keep advancing.



#### Environmental Impact of AI


> **The Environmental Cost of AI:**  
> Artificial intelligence is incredibly powerful, but it comes with a **hidden environmental price**. Take, for example, an AI system by OpenAI that could **solve a Rubik's Cube using a robotic arm**. This task was so complex that it **used 2.8 gigawatt-hours (GWh) of electricity**—the same amount of energy as **running three nuclear reactors for an entire month**. That’s just **one AI experiment**, showing how energy-hungry these systems are.


> **Indian Household Energy Perspective:**  
> In India, the **average person uses about 1,538 kilowatt‑hours (kWh) of electricity per year** — that’s just **1.538 megawatt‑hours (MWh)** per person annually, which is tiny compared to a **single 2.8 GWh AI run**. The energy consumed by that one AI experiment could power **almost 1,820 average Indian households for an entire year** (since 2.8 GWh = 2,800 MWh and 2,800 ÷ 1.538 ≈ 1,820).  [oai_citation:0‡ETEnergyworld.com](https://energy.economictimes.indiatimes.com/news/power/record-surge-in-indias-electricity-consumption-reaches-1538-kwh-per-capita/121756148?utm_source=chatgpt.com)

> **City‑Scale Comparison:**  
> India generates around **1,824 terawatt‑hours (TWh)** of electricity in a year (1 TWh = 1,000 GWh). Even so, **2.8 GWh in AI energy** is like the **daily power needs of a small town or many industrial facilities**, reminding us that large AI workloads are **not trivial when scaled up globally**.  [oai_citation:1‡Wikipedia](https://en.wikipedia.org/wiki/Electricity_sector_in_India?utm_source=chatgpt.com)

> **Renewables and Energy Mix in India:**  
> India is aggressively expanding wind and solar energy to reduce fossil fuel use — with **hundreds of gigawatts of renewable capacity installed** — but much of the power grid still relies on fossil fuels. This means that **energy used for AI currently contributes to carbon emissions**, unless the electricity is from clean sources like solar and wind.  [oai_citation:2‡Next IAS](https://www.nextias.com/ca/editorial-analysis/29-07-2025/india-energy-transition?utm_source=chatgpt.com)





> **Putting it into Perspective:**  
> If we wanted to power **all the AI data centers in the world** using clean energy, we would need about **2.5 million wind turbines**. Since we don’t have that many, most of the electricity comes from **burning fossil fuels** like coal and natural gas. This means that every time we train or run a large AI model, we are **adding carbon emissions to the atmosphere**, contributing to climate change.

> **Why It Matters:**  
> Deep learning models, which are the backbone of AI systems like ChatGPT, image generators, and self-driving cars, are **extremely energy intensive**. As AI becomes more widespread, its **carbon footprint grows**, increasing the strain on the environment. This shows that while AI brings amazing benefits, we also need to **think about sustainable ways to run these systems** to protect our planet.






#### Ethical Considerations in AI
As AI becomes more capable, it is increasingly faced with **complex ethical decisions**. A modern illustration of this is in **self-driving cars**, which must make split-second choices in life-and-death situations. Consider a scenario where the car must **turn left or right**: 

> **Scenario:** Turning left would hit **small children playing with their mother**, while turning right would hit **two elderly pedestrians**. The car must choose **one path**, as continuing straight leads to the driver’s own death.  

>> “I want you to decide. Raise your hand mentally — left or right?

>> There is no correct answer. That’s the problem.

This is a **modern variant of the classic “trolley problem”**, now applied to autonomous vehicles. It highlights that AI is not just a technical challenge—it is also a **moral and societal challenge**, requiring algorithms to weigh consequences, value human life, and follow ethical principles.

> **The Trolley Problem Explained:** 
>“Imagine a runaway trolley (train) is heading toward five people tied on the tracks. You have a lever that can switch the trolley to another track, but there is one person on that track.
>The dilemma is: do you do nothing and let five people die, or pull the lever and let one person die?
>This is called the trolley problem, and it asks a simple question: how should we make moral choices when every option has a cost?”


---

>>>> Respected Chairperson, distinguished guests, fellow academicians, researchers, and participants,
Good morning. I am deeply honored to be invited to deliver the keynote address at this prestigious international seminar. It is a privilege to stand before such a distinguished gathering and share insights on Responsible AI: Towards Ethics, Governance, Scientific and global framework. Today, I hope to give you a perspective on both the opportunities and challenges that lie ahead in this rapidly evolving field, and how we can navigate them responsibly.