---
layout: post
title: "Talk on AI"
date: 2026-01-26 08:00:00 +0530
categories: lecture
tags: Research 
permalink: /AI-Talk/
---

In 1950, Alan Turing published a seminal paper titled "Computing Machinery and Intelligence," in which he asked the provocative question: "Can machines think?" This question laid the foundation for the field of artificial intelligence (AI) and has since sparked decades of research and debate.

It is difficult to formally define "**thinking**," as it will be used as a hard criterion to evaluate whether an AI is thinking or not. However, Turing proposed a practical test, imitation game, now known as the **Turing Test**.

The Turing Test involves a human evaluator who engages in natural language conversations with both a human and a machine, without knowing which is which. If the evaluator cannot reliably distinguish between the human and the machine based on their responses, the machine is said to have passed the Turing Test, demonstrating a form of intelligence.

However, passing the Turing Test does not necessarily imply that a machine possesses true understanding or consciousness. The outcome of the test can be influenced by various factors:

- How long the conversation lasts
- The topics discussed
- The skill of the human participant
- The sophistication of the machine's responses

1956 marked a significant milestone in the history of AI with the Dartmouth Conference in Hanover, New Hampshire, USA, where the term "artificial intelligence" was coined. This conference brought together researchers from various fields, including computer science, psychology, and mathematics, to explore the possibilities of creating intelligent machines.

---

## Understanding the terminology

There is an array of terms associated with AI that are often used interchangeably but have distinct meanings:
- Expert Systems: This example is seen commonly in Telecom and Medical Diagnosis systems. They use **IVR** (Interactive Voice Response) to interact with users and provide expert-level responses based on a predefined set of rules and knowledge bases. When you call a customer support line and navigate through a series of prompts: "Press 1 for english, Press 2 for hindi..." you are interacting with an expert system.

- **Machine Learning (ML):** It is a more sophisticated form of AI that enables systems to learn and improve from experience without being explicitly programmed. Example includes your telecom service provider analyzing your call numbers to classify them as important or spam, and accordingly prioritizing or blocking them.

> A digital photo of a rose is not seen as a “flower” by a computer—it is seen as millions of tiny squares (pixels), and each pixel has numerical values representing color (red, green, blue). One pixel being red does not automatically tell the computer that the next pixel will also be red. To a machine, every pixel is just a number.
> The algorithm looks for patterns of pixels, not individual pixels. By analyzing large datasets of images labeled as "flowers" and "not flowers," the machine learning model can learn to recognize patterns that distinguish flowers from non-flowers, enabling it to classify new images accurately. Over many images, it learns features like edges, curves, petal shapes, and color gradients.



- Neural Networks: These are specialized ML algorithms inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) that process and transmit information.

>  A human neuron receives input signals from other neurons through its dendrites, passes the signals through axons, and transmits them across synapses to other neurons, enabling communication and information processing in the nervous system.

> The rose image example can be extended to neural networks. A neural network processes the pixel data through multiple layers of interconnected nodes, where each layer extracts increasingly complex features from the image. The first layer might detect simple edges, the next layer could identify shapes like petals, and subsequent layers would combine these features to recognize the overall structure of a flower. Through training on large datasets, the neural network learns to associate specific patterns of pixel values with the concept of a "flower," allowing it to classify new images accurately.

- Deep Learning: This is another subset of ML that utilizes multi-layered neural networks to analyze complex patterns in large datasets. Deep learning has been particularly successful in tasks such as image and speech recognition.
 

> Continuing with the rose image example, deep learning involves using neural networks with many layers (hence "deep") to process the pixel data. Each layer of the network learns to extract more abstract and complex features from the image. For instance, the initial layers might focus on basic shapes and colors, while deeper layers could identify intricate details like petal arrangements and textures. By training on vast amounts of labeled images, deep learning models can achieve high accuracy in recognizing flowers, even in varied conditions such as different lighting or angles.

- Generative AI: This refers to AI systems capable of generating new content, such as text, images, or music, based on learned patterns from existing data. Examples include language models like GPT-3 and image generation models like DALL-E. These are specilized deep learning models designed to create new content that resembles human-generated data.

- AGI (Artificial General Intelligence): This represents a theoretical form of AI that possesses human-like cognitive abilities, enabling it to understand, learn, and apply knowledge across a wide range of tasks. AGI remains a goal for future AI research and development. Many experts believe that it will take several decades to achieve AGI, if it is achievable at all. But the the CEO of OpenAI, Sam Altman, has predicted that AGI could be developed as early as 2028.


---

## The Turning Point in the AI Revolution

Two major developments in the last decade fundamentally changed the direction and speed of artificial intelligence. First, in **2015**, Google released **TensorFlow** as an open-source deep learning framework. This made advanced AI tools freely available to researchers, startups, and universities worldwide, dramatically lowering the entry barrier. The second decisive moment came when **OpenAI**, co-founded by *Elon Musk* and led by *Sam Altman*, released **large language models (LLMs)** to the public, most notably **ChatGPT**. For the first time, highly capable AI systems became directly accessible to ordinary users, not just specialists.

The speed of adoption of generative AI has been unprecedented. For perspective, **Netflix took about 3.5 years to reach one million users**. In contrast, **ChatGPT reached one million users in just five days** after launch. Within **two months**, it crossed **100 million users**, making it one of the fastest-growing consumer applications in history. Today, its user base is around **800 millions**, signaling not just popularity but a structural shift in how people interact with technology.

AI is now beginning to influence **politics, law, and global power structures**.

> **News snapshot (Legal incident):**  
> In the United States, several lawyers were **sanctioned by courts for submitting legal briefs containing fake case citations generated by AI tools**. In one widely reported case, attorneys relied on an AI system for legal research and filed documents that cited **non-existent court cases and fabricated judgments**. The sanctions were imposed because the lawyers **failed to verify the AI-generated content**, violating their professional duty to ensure accuracy before submitting documents to the court.

> **What caused the sanctions:**  
> Courts ruled that while AI can assist in research, **responsibility lies entirely with the human lawyer**. The use of AI was not the problem; the problem was **blind trust in AI outputs**, leading to misinformation in official legal proceedings.

> **Broader implication:**  
> These incidents triggered judicial warnings, policy discussions, and renewed emphasis on **AI safety, regulation, and training**, particularly in the U.S., where AI capability is now viewed as a matter of **national competence and global leadership**.

## The Future of AI


> **AI Training at an Incredible Scale:**  
> To understand the scale of modern AI, consider this recent example of a chatbot. Its neural network has **1.8 trillion weights** (for comparison, the human brain has around 100 trillion). It was trained on **13 trillion words** collected from the Internet. The training cost around **$63 million** and took about **five to six months** on powerful supercomputers.

> **Visualizing the Data:**  
> If the 13 trillion words were printed in books of 100,000 words each, and these books were placed on shelves half a metre wide (100 books per shelf), the bookshelves would form a line **650 kilometres long**—almost the distance from **Dhamri to Ranchi**.

> **Visualizing the Model:**  
> Printing all **1.8 trillion weights** on paper would cover **30,000 football fields**. Placed side by side, these fields could stretch all the way from **India to Iran**.  

> **If Using a Regular Laptop:**  
> Training this AI on a normal consumer laptop would take about **7 million years**, showing why **distributed supercomputers** are essential for modern AI development.

>> 650 long book piling points:
>> Moores Law and AI:
>> Environmental Cost of AI:





As AI becomes more capable, it is increasingly faced with **complex ethical decisions**. A modern illustration of this is in **self-driving cars**, which must make split-second choices in life-and-death situations. Consider a scenario where the car must **turn left or right**: 

> **Scenario:** Turning left would hit **small children playing with their mother**, while turning right would hit **two elderly pedestrians**. The car must choose **one path**, as continuing straight leads to the driver’s own death.  

This is a **modern variant of the classic “trolley problem”**, now applied to autonomous vehicles. It highlights that AI is not just a technical challenge—it is also a **moral and societal challenge**, requiring algorithms to weigh consequences, value human life, and follow ethical principles.

> **The Trolley Problem Explained:** 
>“Imagine a runaway trolley (train) is heading toward five people tied on the tracks. You have a lever that can switch the trolley to another track, but there is one person on that track.
>The dilemma is: do you do nothing and let five people die, or pull the lever and let one person die?
>This is called the trolley problem, and it asks a simple question: how should we make moral choices when every option has a cost?”