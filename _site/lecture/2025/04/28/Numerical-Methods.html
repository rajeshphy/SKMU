<!DOCTYPE html>
<!--
    Basically Basic Jekyll Theme 1.4.5
    Copyright 2017-2018 Michael Rose - mademistakes.com | @mmistakes
    Free for personal and commercial use under the MIT license
    https://github.com/mmistakes/jekyll-theme-basically-basic/blob/master/LICENSE
-->
<html lang="en-US" class="no-js">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1, user-scalable=yes">

  
    
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Numerical Methods | Rajesh Kumar</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Numerical Methods" />
<meta name="author" content="Rajesh Kumar" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Eigenvalues and eigenvectors play a central role in linear algebra, with wide applications in physics, engineering, and data science. They help understand the action of a linear transformation in a given vector space." />
<meta property="og:description" content="Eigenvalues and eigenvectors play a central role in linear algebra, with wide applications in physics, engineering, and data science. They help understand the action of a linear transformation in a given vector space." />
<link rel="canonical" href="http://localhost:4000/SKMU/lecture/2025/04/28/Numerical-Methods.html" />
<meta property="og:url" content="http://localhost:4000/SKMU/lecture/2025/04/28/Numerical-Methods.html" />
<meta property="og:site_name" content="Rajesh Kumar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-28T09:47:26+05:30" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/SKMU/lecture/2025/04/28/Numerical-Methods.html"},"author":{"@type":"Person","name":"Rajesh Kumar"},"url":"http://localhost:4000/SKMU/lecture/2025/04/28/Numerical-Methods.html","@type":"BlogPosting","description":"Eigenvalues and eigenvectors play a central role in linear algebra, with wide applications in physics, engineering, and data science. They help understand the action of a linear transformation in a given vector space.","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/SKMU/assets/icons/logo.png"},"name":"Rajesh Kumar"},"headline":"Numerical Methods","dateModified":"2025-04-28T09:47:26+05:30","datePublished":"2025-04-28T09:47:26+05:30","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  

  <script>
    /* Cut the mustard */
    if ( 'querySelector' in document && 'addEventListener' in window ) {
      document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + 'js';
    }
  </script>


  <!-- Include Mermaid.js from a CDN -->
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
  </script>

  <!-- Math JX -->
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
  </script>

  
  <script type="text/javascript">
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        macros: {
          bra: ["{\\langle #1 \\vert}", 1],
          ket: ["{\\vert #1 \\rangle}", 1],
          braket: ["{\\langle #1 \\vert #2 \\rangle}", 2],
          brakett: ["{\\langle #1 \\vert #2 \\vert #3 \\rangle}", 3]
        },
        tags: 'ams' // Enable AMS-style equation numbering

      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>

  








  <link rel="stylesheet" href="/SKMU/assets/stylesheets/main.css">
  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Radio+Canada+Big:400,400i,600,600i">
  

  
    
    <link rel="alternate" type="application/atom+xml" title="Rajesh Kumar" href="https://rajeshphy.github.io/">
  
  <meta name="google-site-verification" content="ONNy_6CkGORdDQ9whxpp_EIkMC_mXOohCfdkEK1zAM8" />
</head>


  <body class="layout--post numerical-methods">

    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#primary-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>


    <div class="sidebar-toggle-wrapper">
      
        <button class="search-toggle" type="button">
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <title>Search</title>
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
      

      <button class="toggle navicon-button larr" type="button">
        <span class="toggle-inner">
          <span class="sidebar-toggle-label visually-hidden">Menu</span>
          <span class="navicon"></span>
        </span>
      </button>
    </div>

    <div id="sidebar" class="sidebar">
      <div class="inner">
        <nav id="primary-nav" class="site-nav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
  <ul id="menu-main-navigation" class="menu">
    <!-- Home link -->
    <li class="menu-item">
      <a href="/SKMU/" itemprop="url">
        <span itemprop="name">Home</span>
      </a>
    </li>

    <!-- site.pages links -->
    
    

    
      
      
    
      
      
    
      
      
        <li class="menu-item">
          <a href="/SKMU/posts/" itemprop="url">
            <span itemprop="name">Posts</span>
          </a>
        </li>
      
    
  </ul>
</nav>

        <ul class="contact-list">
  
    <li>
      <a href="mailto:kr.rajesh.phy@gmail.com">
        <span class="icon icon--email"><svg xmlns="http://www.w3.org/2000/svg" viewBox="313.1 3.7 16 16"><path d="M318.5 8.9c0-.2.2-.4.4-.4h4.5c.2 0 .4.2.4.4s-.2.4-.4.4h-4.5c-.3 0-.4-.2-.4-.4zm.4 2.1h4.5c.2 0 .4-.2.4-.4s-.2-.4-.4-.4h-4.5c-.2 0-.4.2-.4.4s.1.4.4.4zm3.5 1.2c0-.2-.2-.4-.4-.4h-3.1c-.2 0-.4.2-.4.4s.2.4.4.4h3.1c.2.1.4-.1.4-.4zm-1.5-8.4l-1.7 1.4c-.2.1-.2.4 0 .6s.4.2.6 0l1.4-1.2 1.4 1.2c.2.1.4.1.6 0s.1-.4 0-.6l-1.7-1.4c-.3-.1-.5-.1-.6 0zm7.8 6.2c.1.1.1.2.1.3v7.9c0 .8-.7 1.5-1.5 1.5h-12.5c-.8 0-1.5-.7-1.5-1.5v-7.9c0-.1.1-.2.1-.3l1.6-1.3c.2-.1.4-.1.6 0s.1.4 0 .6l-1.2 1 1.8 1.3v-4c0-.6.5-1.1 1.1-1.1h7.5c.6 0 1.1.5 1.1 1.1v4l1.8-1.3-1.2-1c-.2-.1-.2-.4 0-.6s.4-.2.6 0l1.6 1.3zm-11.6 2.2l4 2.8 4-2.8V7.6c0-.1-.1-.2-.2-.2h-7.5c-.1 0-.2.1-.2.2v4.6zm10.9-1l-4.7 3.4 3.4 2.6c.2.1.2.4.1.6-.1.2-.4.2-.6.1l-3.6-2.8-1.2.8c-.1.1-.3.1-.5 0l-1.2-.8-3.6 2.8c-.2.1-.4.1-.6-.1-.1-.2-.1-.4.1-.6l3.4-2.6-4.7-3.4v7.1c0 .4.3.6.6.6h12.5c.4 0 .6-.3.6-.6v-7.1z"/></svg></span>
        <span class="label">Email</span>
      </a>
    </li>
  

  

  

  <li>
    
      <a href="https://rajeshphy.github.io/" title="Atom Feed">
        <span class="icon icon--rss"><svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194 11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0 13.806c0-1.21.983-2.195 2.194-2.195zM10.606 16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"/></svg></span>
        <span class="label">Subscribe</span>
      </a>
    
  </li>
</ul>

      </div>
    </div>

    <div class="canvas">
      <div class="wrapper">
        

<header id="masthead">
  <div class="inner">
    <div class="title-area">
      
        <p class="site-title">
          <a href="/SKMU/">
            <img src="/SKMU/assets/icons/logo.png" alt="" class="site-logo">
            <span>Rajesh Kumar</span>
          </a>
        </p>
      
    </div>
  </div>
</header>

        <div class="initial-content">
          <header class="intro">
  

  <div class="inner">
    <div class="intro-text">
      <h1 id="page-title" class="intro-title">Numerical Methods
</h1>
      

      
        


        <p class="entry-meta">
          <span class="byline-item">by Rajesh Kumar</span><span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="379 72 16 16"><g><g><path fill="none" d="M380.8,86.7h12.3v-8.8h-12.3V86.7z M389.5,78.8h1.7v1.4h-1.7V78.8z M389.5,81.3h1.7v1.4h-1.7V81.3z M389.5,83.8h1.7v1.4h-1.7V83.8z M386.1,78.8h1.7v1.4h-1.7V78.8z M386.1,81.3h1.7v1.4h-1.7V81.3z M386.1,83.8h1.7v1.4h-1.7V83.8z M382.8,78.8h1.7v1.4h-1.7V78.8z M382.8,81.3h1.7v1.4h-1.7V81.3z M382.8,83.8h1.7v1.4h-1.7V83.8z"/><polygon fill="none" points="384.7 75.1 383.4 75.1 383.4 74.3 380.8 74.3 380.8 76.6 393.2 76.6 393.2 74.3 390.6 74.3 390.6 75.1 389.3 75.1 389.3 74.3 384.7 74.3"/><rect x="382.8" y="78.8" width="1.7" height="1.4"/><rect x="386.1" y="78.8" width="1.7" height="1.4"/><rect x="389.5" y="78.8" width="1.7" height="1.4"/><rect x="382.8" y="81.3" width="1.7" height="1.4"/><rect x="386.1" y="81.3" width="1.7" height="1.4"/><rect x="389.5" y="81.3" width="1.7" height="1.4"/><rect x="382.8" y="83.8" width="1.7" height="1.4"/><rect x="386.1" y="83.8" width="1.7" height="1.4"/><rect x="389.5" y="83.8" width="1.7" height="1.4"/><path d="M383.4,72v1.1h-3.8V88h14.9V73.1h-3.8V72h-1.3v1.1h-4.7V72H383.4z M393.2,86.7h-12.3v-8.8h12.3L393.2,86.7L393.2,86.7z M389.3,74.3v0.8h1.3v-0.8h2.5v2.3h-12.3v-2.3h2.5v0.8h1.3v-0.8H389.3z"/></g></g></svg></span><time datetime="2025-04-28T09:47:26+05:30">April 28, 2025</time></span> <span class="byline-item"><span class="icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="15 309.7 16 16"><g><path d="M23.9 315.1v3.6c0 .5-.4.9-.9.9s-.9-.4-.9-.9v-3.6h1.8z"/><path d="M30.1 317.7c.5 3.9-2.3 7.5-6.2 7.9-3.9.5-7.5-2.3-7.9-6.2-.5-3.9 2.3-7.5 6.2-7.9v-1.8H24v1.8c1.1.1 2.7.7 3.5 1.4l1.3-1.3 1.3 1.3-1.3 1.3c.5.9 1.2 2.5 1.3 3.5zm-1.8.9c0-2.9-2.4-5.3-5.3-5.3s-5.3 2.4-5.3 5.3 2.4 5.3 5.3 5.3 5.3-2.3 5.3-5.3z"/></g></svg></span>14 min read</span>
        </p>
      

      

      
    </div>
  </div>
</header>


<main id="main" class="page-content" aria-label="Content">
  <div class="inner">
    <article class="entry-wrap">
      <div class="entry-content">
        <p>Eigenvalues and eigenvectors play a central role in linear algebra, with wide applications in physics, engineering, and data science. They help understand the action of a linear transformation in a given vector space.</p>

<hr />

<h2 id="-basic-definitions">🔹 Basic Definitions</h2>

<p>Let \(A\) be an \(n \times n\) square matrix. A non-zero vector \(\mathbf{v} \in \mathbb{R}^n\) is called an <strong>eigenvector</strong> of \(A\) if it satisfies:</p>

\[A \mathbf{v} = \lambda \mathbf{v}\]

<p>Here:</p>

<ul>
  <li>\(\lambda \in \mathbb{R}\) (or \(\mathbb{C}\)) is the <strong>eigenvalue</strong> corresponding to eigenvector \(\mathbf{v}\).</li>
  <li>\(\mathbf{v} \ne \mathbf{0}\) is a <strong>direction preserved</strong> under the transformation by \(A\), scaled by \(\lambda\).</li>
</ul>

<hr />

<h2 id="-how-to-find-eigenvalues-and-eigenvectors">🔹 How to Find Eigenvalues and Eigenvectors</h2>

<h3 id="step-1-characteristic-equation">Step 1: Characteristic Equation</h3>

<p>To find eigenvalues, solve the <strong>characteristic equation</strong>:</p>

\[\det(A - \lambda I) = 0\]

<ul>
  <li>\(I\) is the identity matrix of the same size as \(A\).</li>
  <li>The determinant gives a polynomial in \(\lambda\) called the <strong>characteristic polynomial</strong>.</li>
</ul>

<h3 id="step-2-solve-for-eigenvectors">Step 2: Solve for Eigenvectors</h3>

<p>For each eigenvalue \(\lambda\), solve the system:</p>

\[(A - \lambda I) \mathbf{v} = 0\]

<p>to find the corresponding eigenvector(s) \(\mathbf{v}\).</p>

<hr />

<h2 id="-example">🔸 Example</h2>

<p>Let</p>

\[A = \begin{bmatrix}
2 &amp; 1 \\
1 &amp; 2
\end{bmatrix}\]

<h3 id="step-1-find-eigenvalues">Step 1: Find Eigenvalues</h3>

<p>Solve:</p>

\[\det(A - \lambda I) = \det \begin{bmatrix}
2 - \lambda &amp; 1 \\
1 &amp; 2 - \lambda
\end{bmatrix}
= (2 - \lambda)^2 - 1 = 0\]

<p>So,</p>

\[(2 - \lambda)^2 = 1 \Rightarrow \lambda = 1, 3\]

<h3 id="step-2-find-eigenvectors">Step 2: Find Eigenvectors</h3>

<p>For \(\lambda = 1\):</p>

\[(A - I) \mathbf{v} = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{bmatrix} \begin{bmatrix}
x \\
y
\end{bmatrix} = 0
\Rightarrow x + y = 0 \Rightarrow \mathbf{v}_1 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}\]

<p>For \(\lambda = 3\):</p>

\[(A - 3I) \mathbf{v} = \begin{bmatrix}
-1 &amp; 1 \\
1 &amp; -1
\end{bmatrix} \begin{bmatrix}
x \\
y
\end{bmatrix} = 0
\Rightarrow x - y = 0 \Rightarrow \mathbf{v}_2 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}\]

<hr />

<h2 id="-key-properties">🔹 Key Properties</h2>

<ul>
  <li>A matrix of size \(n \times n\) has at most \(n\) eigenvalues.</li>
  <li>Eigenvectors corresponding to <strong>distinct eigenvalues are linearly independent</strong>.</li>
  <li>If a matrix is <strong>symmetric</strong>, all its eigenvalues are real and eigenvectors are orthogonal.</li>
</ul>

<hr />

<h2 id="-physical-interpretation">🔹 Physical Interpretation</h2>

<p>In physics:</p>

<ul>
  <li>In quantum mechanics, eigenvalues of operators represent <strong>observable quantities</strong>.</li>
  <li>In mechanics, the <strong>normal modes</strong> of oscillation are eigenvectors of the system matrix.</li>
</ul>

<hr />

<h2 id="-summary">📌 Summary</h2>

<table>
  <thead>
    <tr>
      <th>Term</th>
      <th>Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Eigenvalue</td>
      <td>Scalar \(\lambda\) such that \(A \mathbf{v} = \lambda \mathbf{v}\)</td>
    </tr>
    <tr>
      <td>Eigenvector</td>
      <td>Non-zero vector \(\mathbf{v}\) preserved in direction by \(A\)</td>
    </tr>
    <tr>
      <td>Characteristic Equation</td>
      <td>\(\det(A - \lambda I) = 0\) to find eigenvalues</td>
    </tr>
    <tr>
      <td>Matrix Diagonalization</td>
      <td>Possible if matrix has \(n\) linearly independent eigenvectors</td>
    </tr>
  </tbody>
</table>

<hr />
<h1 id="-interpolation-extrapolation-and-curve-fitting">📘 Interpolation, Extrapolation, and Curve Fitting</h1>

<hr />

<h2 id="-1-interpolation">🔹 1. Interpolation</h2>

<h3 id="-definition">🔸 Definition:</h3>

<p>Interpolation is the process of estimating unknown values that fall <strong>within the range</strong> of known data points.</p>

<h3 id="-types-of-interpolation">🔸 Types of Interpolation:</h3>

<ul>
  <li><strong>Linear Interpolation</strong>: Straight line between two known points.</li>
  <li><strong>Polynomial Interpolation</strong>: Uses a polynomial of degree $n$ for $n+1$ data points.</li>
  <li><strong>Spline Interpolation</strong>: Piecewise polynomials (e.g., cubic spline) to ensure smoothness.</li>
</ul>

<h3 id="-formula-linear-interpolation">🔸 Formula (Linear Interpolation):</h3>

<p>Given two points $(x_0, y_0)$ and $(x_1, y_1)$:</p>

\[y = y_0 + \frac{(x - x_0)(y_1 - y_0)}{x_1 - x_0}\]

<h3 id="-example-linear-interpolation">🔸 Example (Linear Interpolation):</h3>

<p>Let $(x_0, y_0) = (1, 3)$ and $(x_1, y_1) = (4, 15)$. Estimate $y$ at $x = 2$.</p>

\[y = 3 + \frac{(2 - 1)(15 - 3)}{4 - 1} = 3 + \frac{1 \cdot 12}{3} = 3 + 4 = 7\]

<p>So, the interpolated value at $x = 2$ is $y = 7$.</p>

<h3 id="-applications">🔸 Applications:</h3>

<ul>
  <li>Filling missing data</li>
  <li>Digital image scaling</li>
  <li>Sensor data smoothing</li>
</ul>

<hr />

<h2 id="️-2-extrapolation">🗙️ 2. Extrapolation</h2>

<h3 id="-definition-1">🔸 Definition:</h3>

<p>Extrapolation estimates values <strong>outside the range</strong> of known data points using the trend of the data.</p>

<h3 id="-types">🔸 Types:</h3>

<ul>
  <li><strong>Linear Extrapolation</strong>: Extends the linear trend.</li>
  <li><strong>Polynomial Extrapolation</strong>: Uses higher-order polynomials to forecast.</li>
</ul>

<h3 id="-risks">🔸 Risks:</h3>

<ul>
  <li>Less reliable than interpolation.</li>
  <li>Assumes the current trend continues.</li>
</ul>

<h3 id="-example-linear">🔸 Example (Linear):</h3>

<p>Given last two points: $(x_{n-1}, y_{n-1}) = (2, 5)$ and $(x_n, y_n) = (4, 11)$, estimate $y$ at $x = 5$.</p>

\[y = 11 + (5 - 4) \cdot \frac{11 - 5}{4 - 2} = 11 + 1 \cdot 3 = 14\]

<p>So, the extrapolated value at $x = 5$ is $y = 14$.</p>

<hr />

<h2 id="-3-curve-fitting-methods">🔹 3. Curve Fitting Methods</h2>

<h3 id="-definition-2">🔸 Definition:</h3>

<p>Curve fitting finds a curve that best represents the trend in the data. It can be used to model the relationship between variables.</p>

<h3 id="-methods">🔸 Methods:</h3>

<ul>
  <li><strong>Polynomial Fit</strong>: Fit using polynomials (linear, quadratic, cubic, etc.).</li>
  <li><strong>Exponential Fit</strong>: $y = ae^{bx}$</li>
  <li><strong>Logarithmic Fit</strong>: $y = a + b \log x$</li>
  <li><strong>Power Law Fit</strong>: $y = ax^b$</li>
  <li><strong>Piecewise Fit</strong>: Different models in different intervals.</li>
</ul>

<h3 id="-example-polynomial-fit">🔸 Example (Polynomial Fit):</h3>

<p>Given data: $(1, 2)$, $(2, 4.1)$, $(3, 6.2)$</p>

<p>Fit a line: $y = mx + c$ using least squares:</p>

<ul>
  <li>Normal equations lead to $m \approx 2.1$, $c \approx -0.1$</li>
</ul>

<p>So, best-fit line: $y = 2.1x - 0.1$</p>

<h3 id="-purpose">🔸 Purpose:</h3>

<ul>
  <li>Data modeling</li>
  <li>Predictive analytics</li>
  <li>Simplification of complex datasets</li>
</ul>

<h3 id="-tools">🔸 Tools:</h3>

<ul>
  <li>Manual fitting</li>
  <li>Python libraries: NumPy (<code class="language-plaintext highlighter-rouge">polyfit</code>), SciPy, Matplotlib</li>
  <li>MATLAB, Excel</li>
</ul>

<hr />

<h2 id="-4-least-squares-fitting">🔹 4. Least Squares Fitting</h2>

<h3 id="-definition-3">🔸 Definition:</h3>

<p>The least squares method minimizes the <strong>sum of the squares of the vertical differences</strong> (residuals) between the observed and predicted values.</p>

<h3 id="-linear-least-squares">🔸 Linear Least Squares:</h3>

<p>Given data points $(x_i, y_i)$, find $y = mx + c$ that minimizes:</p>

\[S = \sum_{i=1}^n (y_i - (mx_i + c))^2\]

<h3 id="-example-linear-least-squares-fit">🔸 Example (Linear Least Squares Fit):</h3>

<p>Data: $(1,2)$, $(2,3)$, $(3,5)$</p>

<p>Compute:</p>

<ul>
  <li>$\sum x = 6$, $\sum y = 10$, $\sum xy = 23$, $\sum x^2 = 14$, $n=3$</li>
</ul>

<p>Normal equations:</p>

\[10 = 6m + 3c \\
23 = 14m + 6c\]

<p>Solving gives: $m = 1.5$, $c = 0.333$</p>

<p>Best fit: $y = 1.5x + 0.333$</p>

<h3 id="-polynomial-least-squares">🔸 Polynomial Least Squares:</h3>

<p>Minimize the sum of squares for a polynomial:</p>

\[y = a_0 + a_1x + a_2x^2 + \dots + a_nx^n\]

<p>Use matrix techniques to solve the normal equations.</p>

<h3 id="-advantages">🔸 Advantages:</h3>

<ul>
  <li>Simple to implement</li>
  <li>Well-studied and robust</li>
</ul>

<h3 id="-limitations">🔸 Limitations:</h3>

<ul>
  <li>Sensitive to outliers</li>
  <li>Overfitting with high-degree polynomials</li>
</ul>

<hr />

<h2 id="-summary-table">📋 Summary Table</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Domain</th>
      <th>Input Data Range</th>
      <th>Output Estimate</th>
      <th>Confidence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Interpolation</td>
      <td>Within data</td>
      <td>[x_min, x_max]</td>
      <td>Estimated y</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Extrapolation</td>
      <td>Outside data</td>
      <td>x &lt; x_min or x &gt; x_max</td>
      <td>Forecasted y</td>
      <td>Lower</td>
    </tr>
    <tr>
      <td>Curve Fitting</td>
      <td>Entire dataset</td>
      <td>All data points</td>
      <td>Best-fit curve (y vs x)</td>
      <td>Varies</td>
    </tr>
    <tr>
      <td>Least Squares</td>
      <td>Numerical method</td>
      <td>All data points</td>
      <td>Curve parameters (e.g. m, c)</td>
      <td>Depends on data</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-further-reading">🔍 Further Reading</h2>

<ul>
  <li>Numerical Analysis by R.L. Burden and J.D. Faires</li>
  <li>Curve Fitting for Programmers (NumPy, SciPy)</li>
  <li>Applied Regression Analysis</li>
</ul>

<h1 id="solution-of-first-order-differential-equation-using-runge-kutta-method">Solution of First Order Differential Equation using Runge-Kutta Method</h1>

<p>The numerical solution of <strong>first-order differential equations</strong> plays a crucial role across science and engineering. While many analytical methods exist, they are often limited to relatively simple equations. As a result, numerical methods have become indispensable tools for approximating solutions.</p>

<p>Several methods are available for numerically solving first-order ordinary differential equations (ODEs):</p>

<ul>
  <li><strong>Euler’s Method</strong>: The simplest method, based on a first-order Taylor expansion. It is easy to implement but often suffers from large truncation errors, especially over larger step sizes.</li>
  <li><strong>Improved Euler’s Method (Heun’s Method)</strong>: An enhancement over Euler’s method, reducing errors by using an averaged slope.</li>
  <li><strong>Taylor Series Methods</strong>: These methods provide high accuracy but require the calculation of higher-order derivatives, making them computationally intensive.</li>
  <li><strong>Runge-Kutta Methods</strong>: A family of iterative methods that achieve higher accuracy without requiring higher derivatives. They are the most widely used in practice due to their balance between simplicity, accuracy, and computational efficiency.</li>
</ul>

<p>Among these, the <strong>Runge-Kutta methods</strong> stand out as the most popular because:</p>

<ul>
  <li>They do not require the explicit computation of higher derivatives (unlike Taylor series methods).</li>
  <li>They can achieve high-order accuracy with relatively simple formulas.</li>
  <li>They are robust and flexible, applicable to a wide variety of differential equations.</li>
</ul>

<p>The Runge-Kutta family includes methods of various orders:</p>

<ul>
  <li><strong>First-Order Runge-Kutta (RK1)</strong>: Equivalent to Euler’s method.</li>
  <li><strong>Second-Order Runge-Kutta (RK2)</strong>: Also known as the Improved Euler or Heun’s method, offering better accuracy.</li>
  <li><strong>Third-Order Runge-Kutta (RK3)</strong>: Provides intermediate accuracy but is less commonly used.</li>
  <li><strong>Fourth-Order Runge-Kutta (RK4)</strong>: The most popular method, offering excellent accuracy with manageable computational complexity.</li>
  <li><strong>Higher-Order Runge-Kutta Methods</strong>: Methods of order five and above exist (such as the Runge-Kutta-Fehlberg and Dormand-Prince methods) but are typically used for adaptive step-size control in more advanced applications.</li>
</ul>

<hr />

<h2 id="fourth-order-runge-kutta-method-rk4">Fourth-Order Runge-Kutta Method (RK4)</h2>

<p>The <strong>Runge-Kutta methods</strong> are a family of iterative methods for approximating the solution of <strong>first-order ordinary differential equations (ODEs)</strong> of the form:</p>

\[\frac{dy}{dx} = f(x, y), \quad y(x_0) = y_0\]

<p>Suppose we wish to find \(y(x)\) at \(x = x_0 + h\) given \(y(x_0) = y_0\). The RK4 method uses the following steps:</p>

<h3 id="formulae">Formulae:</h3>

<p>Compute intermediate slopes:</p>

\[\begin{aligned}
k_1 &amp;= h f(x_0, y_0) \\
k_2 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_1}{2}\right) \\
k_3 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_2}{2}\right) \\
k_4 &amp;= h f(x_0 + h, y_0 + k_3)
\end{aligned}\]

<p>Then, update the solution:</p>

\[y(x_0+h) = y_0 + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4)\]

<hr />

<h2 id="step-by-step-procedure">Step-by-Step Procedure</h2>

<ol>
  <li>Start with initial conditions \((x_0, y_0)\).</li>
  <li>Choose a step size \(h\).</li>
  <li>Compute \(k_1, k_2, k_3, k_4\) using the given \(f(x, y)\).</li>
  <li>Find the next value \(y_1\) using the weighted average.</li>
  <li>Update \(x\) to \(x_1 = x_0 + h\).</li>
  <li>Repeat the process as needed.</li>
</ol>

<hr />

<h2 id="example-1">Example 1</h2>

<h3 id="problem">Problem:</h3>

<p>Solve</p>

\[\frac{dy}{dx} = x + y, \quad y(0) = 1\]

<p>Find \(y(0.1)\) using RK4 with step size \(h = 0.1\).</p>

<hr />

<h3 id="solution">Solution:</h3>

<p>Given:</p>

\[f(x,y) = x + y\]

<p>Initial conditions:</p>

\[x_0 = 0, \quad y_0 = 1, \quad h = 0.1\]

<p>Compute:</p>

\[\begin{aligned}
k_1 &amp;= h f(x_0, y_0) = 0.1 (0 + 1) = 0.1 \\
k_2 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_1}{2}\right) = 0.1 (0.05 + 1.05) = 0.1(1.1) = 0.11 \\
k_3 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_2}{2}\right) = 0.1 (0.05 + 1.055) = 0.1(1.105) = 0.1105 \\
k_4 &amp;= h f(x_0 + h, y_0 + k_3) = 0.1 (0.1 + 1.1105) = 0.1(1.2105) = 0.12105
\end{aligned}\]

<p>Now:</p>

\[\begin{aligned}
y(0.1) &amp;= y_0 + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\
&amp;= 1 + \frac{1}{6}(0.1 + 2(0.11) + 2(0.1105) + 0.12105) \\
&amp;= 1 + \frac{1}{6}(0.1 + 0.22 + 0.221 + 0.12105) \\
&amp;= 1 + \frac{1}{6}(0.66205) \\
&amp;= 1 + 0.11034 \\
&amp;\approx 1.11034
\end{aligned}\]

<p>Thus, \(y(0.1) \approx 1.11034\).</p>

<hr />

<h2 id="example-2">Example 2</h2>

<h3 id="problem-1">Problem:</h3>

<p>Solve</p>

\[\frac{dy}{dx} = y - x^2 + 1, \quad y(0) = 0.5\]

<p>Find \(y(0.2)\) using RK4 with step size \(h = 0.2\).</p>

<hr />

<h3 id="solution-1">Solution:</h3>

<p>Given:</p>

\[f(x,y) = y - x^2 + 1\]

<p>Initial conditions:</p>

\[x_0 = 0, \quad y_0 = 0.5, \quad h = 0.2\]

<p>Compute:</p>

\[\begin{aligned}
k_1 &amp;= h f(x_0, y_0) = 0.2 (0.5 - 0^2 + 1) = 0.2(1.5) = 0.3 \\
k_2 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_1}{2}\right) = 0.2\left( (0.5 + 0.15) - (0.1)^2 + 1 \right) \\
&amp;= 0.2 (0.65 - 0.01 + 1) = 0.2(1.64) = 0.328 \\
k_3 &amp;= h f\left(x_0 + \frac{h}{2}, y_0 + \frac{k_2}{2}\right) = 0.2 \left( (0.5 + 0.164) - (0.1)^2 + 1 \right) \\
&amp;= 0.2(1.654) = 0.3308 \\
k_4 &amp;= h f(x_0 + h, y_0 + k_3) = 0.2 \left( (0.5 + 0.3308) - (0.2)^2 + 1 \right) \\
&amp;= 0.2(0.8308 - 0.04 + 1) = 0.2(1.7908) = 0.35816
\end{aligned}\]

<p>Now:</p>

\[\begin{aligned}
y(0.2) &amp;= y_0 + \frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\
&amp;= 0.5 + \frac{1}{6}(0.3 + 2(0.328) + 2(0.3308) + 0.35816) \\
&amp;= 0.5 + \frac{1}{6}(0.3 + 0.656 + 0.6616 + 0.35816) \\
&amp;= 0.5 + \frac{1}{6}(1.97576) \\
&amp;= 0.5 + 0.32929 \\
&amp;= 0.82929
\end{aligned}\]

<p>Thus, \(y(0.2) \approx 0.82929\).</p>

<hr />

<h2 id="advantages-of-runge-kutta-method">Advantages of Runge-Kutta Method</h2>

<ul>
  <li>High accuracy with relatively fewer steps.</li>
  <li>No need to calculate higher derivatives (unlike Taylor series method).</li>
  <li>Widely applicable to a variety of ODE problems.</li>
</ul>

<hr />
<hr />

<h1 id="finite-difference-method-fdm">Finite Difference Method (FDM)</h1>

<h2 id="introduction">Introduction</h2>

<p>The <strong>Finite Difference Method (FDM)</strong> is one of the most widely used numerical techniques for solving differential equations, particularly when analytical solutions are difficult or impossible to obtain.</p>

<p>Compared to other methods:</p>

<ul>
  <li><strong>Simplicity</strong>: FDM directly discretizes the differential equations into algebraic equations, making it easy to implement.</li>
  <li><strong>Flexibility</strong>: It can handle complex boundary conditions effectively.</li>
  <li><strong>Efficiency</strong>: It is computationally faster for structured grids and simple geometries.</li>
  <li><strong>Accuracy Control</strong>: The accuracy can be systematically improved by refining the grid (reducing step size).</li>
</ul>

<p>Unlike methods like the <strong>Taylor series expansion</strong> (which require computation of higher-order derivatives) or the <strong>Runge-Kutta methods</strong> (which approximate solutions point by point), FDM transforms the entire differential equation into a system of algebraic equations across a discretized domain, providing a <strong>global</strong> numerical solution.</p>

<p>Thus, FDM is particularly powerful for solving:</p>
<ul>
  <li><strong>Boundary Value Problems</strong> (BVPs)</li>
  <li><strong>Partial Differential Equations</strong> (PDEs)</li>
  <li><strong>Time-dependent problems</strong> (in combination with time discretization)</li>
</ul>

<hr />

<h2 id="working-principle">Working Principle</h2>

<p>The core idea of the Finite Difference Method is to replace <strong>derivatives</strong> by <strong>finite difference approximations</strong>.</p>

<p>For a function \(y(x)\), the derivatives are approximated as:</p>

<ul>
  <li><strong>First Derivative</strong> (Forward Difference):
\(\frac{dy}{dx}\Bigg|_{x=x_i} \approx \frac{y(x_{i+1}) - y(x_i)}{h}\)</li>
  <li><strong>First Derivative</strong> (Backward Difference):
\(\frac{dy}{dx}\Bigg|_{x=x_i} \approx \frac{y(x_i) - y(x_{i-1})}{h}\)</li>
  <li>
    <p><strong>First Derivative</strong> (Central Difference):
\(\frac{dy}{dx}\Bigg|_{x=x_i} \approx \frac{y(x_{i+1}) - y(x_{i-1})}{2h}\)</p>
  </li>
  <li><strong>Second Derivative</strong> (Central Difference):
\(\frac{d^2y}{dx^2}\Bigg|_{x=x_i} \approx \frac{y(x_{i+1}) - 2y(x_i) + y(x_{i-1})}{h^2}\)</li>
</ul>

<p>where:</p>
<ul>
  <li>\(h\) is the step size between adjacent points: \(h = x_{i+1} - x_i\).</li>
  <li>\(x_i\) are the grid points at which we compute the solution.</li>
</ul>

<p><strong>Basic steps:</strong></p>
<ol>
  <li>Discretize the domain into a set of points.</li>
  <li>Replace derivatives in the differential equation using finite differences.</li>
  <li>Form a system of algebraic equations.</li>
  <li>Solve the system to approximate the values of the unknown function at the grid points.</li>
</ol>

<hr />

<h2 id="simple-example">Simple Example</h2>

<h3 id="example-solve">Example: Solve</h3>

<p>\(\frac{d^2y}{dx^2} = -2, \quad 0 \leq x \leq 1\)
with boundary conditions:</p>

\[y(0) = 0, \quad y(1) = 0\]

<hr />

<h3 id="step-1-discretize-the-domain">Step 1: Discretize the domain</h3>

<p>Let’s divide the domain into 4 equal intervals (5 points):</p>

\[h = \frac{1-0}{4} = 0.25\]

<p>Grid points:</p>

\[x_0 = 0, \quad x_1 = 0.25, \quad x_2 = 0.5, \quad x_3 = 0.75, \quad x_4 = 1\]

<p>Given: \(y(0) = 0\), \(y(1) = 0\)</p>

<p>We need to find \(y_1, y_2, y_3\).</p>

<hr />

<h3 id="step-2-replace-derivatives-using-finite-difference-approximation">Step 2: Replace derivatives using finite difference approximation</h3>

<p>Using central difference for the second derivative:</p>

\[\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} = -2\]

<p>Multiply throughout by \(h^2\):</p>

\[y_{i+1} - 2y_i + y_{i-1} = -2h^2\]

<p>Since \(h = 0.25\), we have:</p>

\[h^2 = 0.0625\]

<p>Thus:</p>

\[y_{i+1} - 2y_i + y_{i-1} = -0.125\]

<hr />

<h3 id="step-3-set-up-equations">Step 3: Set up equations</h3>

<p>For each interior point:</p>

<ul>
  <li>
    <p>At \(x_1\): 
\(y_2 - 2y_1 + y_0 = -0.125\)
Since \(y_0 = 0\), it simplifies to:
\(y_2 - 2y_1 = -0.125\)</p>
  </li>
  <li>
    <p>At \(x_2\):
\(y_3 - 2y_2 + y_1 = -0.125\)</p>
  </li>
  <li>
    <p>At \(x_3\):
\(y_4 - 2y_3 + y_2 = -0.125\)
Since \(y_4 = 0\), it simplifies to:
\(-2y_3 + y_2 = -0.125\)</p>
  </li>
</ul>

<hr />

<h3 id="step-4-solve-the-system">Step 4: Solve the system</h3>

<p>System of equations:</p>

\[\begin{aligned}
-2y_1 + y_2 &amp;= -0.125 \quad (1) \\
y_1 - 2y_2 + y_3 &amp;= -0.125 \quad (2) \\
y_2 - 2y_3 &amp;= -0.125 \quad (3)
\end{aligned}\]

<p>You can solve this system using substitution, matrix methods, or a simple calculator to find \(y_1, y_2, y_3\).</p>

<hr />
<hr />

<h1 id="numerical-integration-trapezoidal-rule-and-simpsons-rule">Numerical Integration: Trapezoidal Rule and Simpson’s Rule</h1>

<p>In many practical situations, finding the exact value of a definite integral:</p>

\[\int_a^b f(x)\,dx\]

<p>is either very difficult or impossible analytically.<br />
<strong>Numerical integration (or quadrature)</strong> techniques provide approximate methods to evaluate such integrals.</p>

<p>Two of the most popular and widely used methods are:</p>

<ul>
  <li><strong>Trapezoidal Rule</strong></li>
  <li><strong>Simpson’s Rule</strong></li>
</ul>

<p>Both methods replace the function with simple polynomials (linear for trapezoidal, quadratic for Simpson’s) and then integrate the approximations exactly.</p>

<h2 id="trapezoidal-rule">Trapezoidal Rule</h2>

<p>The <strong>Trapezoidal Rule</strong> approximates the area under a curve by dividing it into <strong>trapezoids</strong> instead of rectangles.</p>

<p>Suppose we want to evaluate:</p>

\[I = \int_a^b f(x)\,dx\]

<p>Divide the interval \([a, b]\) into \(n\) equal subintervals, each of width:</p>

\[h = \frac{b-a}{n}\]

<p>The trapezoidal approximation is:</p>

\[I \approx \frac{h}{2} \left[ f(x_0) + 2f(x_1) + 2f(x_2) + \cdots + 2f(x_{n-1}) + f(x_n) \right]\]

<p>where:</p>

<ul>
  <li>
\[x_0 = a\]
  </li>
  <li>
\[x_n = b\]
  </li>
  <li>\(x_i = a + ih\) for \(i = 1, 2, \ldots, n-1\)</li>
</ul>

<h3 id="error-estimate">Error Estimate</h3>

<p>The error \(E_T\) in the trapezoidal rule is approximately:</p>

\[E_T = -\frac{(b-a)^3}{12n^2} f''(\xi)\]

<p>for some \(\xi\) in \((a,b)\).<br />
Thus, the error decreases quadratically as \(n\) increases.</p>

<h2 id="simpsons-rule">Simpson’s Rule</h2>

<p><strong>Simpson’s Rule</strong> approximates the function by a <strong>second-degree polynomial (parabola)</strong> through each set of three points and integrates the parabola exactly.</p>

<p>Divide \([a, b]\) into an <strong>even</strong> number \(n\) of subintervals (important: \(n\) must be even), each of width:</p>

\[h = \frac{b-a}{n}\]

<p>The Simpson’s 1/3 Rule formula is:</p>

\[I \approx \frac{h}{3} \left[ f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + 2f(x_4) + \cdots + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n) \right]\]

<p>Notice the pattern:</p>
<ul>
  <li>Coefficient 4 for odd-indexed points</li>
  <li>Coefficient 2 for even-indexed points (except first and last)</li>
</ul>

<h3 id="error-estimate-1">Error Estimate</h3>

<p>The error \(E_S\) in Simpson’s Rule is approximately:</p>

\[E_S = -\frac{(b-a)^5}{180n^4} f^{(4)}(\xi)\]

<p>for some \(\xi\) in \((a,b)\).<br />
Thus, Simpson’s rule is much more accurate than the trapezoidal rule for smooth functions — error decreases with \(n^4\).</p>

<hr />

<h2 id="simple-examples">Simple Examples</h2>

<h3 id="example-1-trapezoidal-rule">Example 1: Trapezoidal Rule</h3>

<p>Approximate:</p>

\[\int_0^1 x^2\,dx\]

<p>using \(n=2\) intervals.</p>

<h4 id="step-1-divide-the-interval">Step 1: Divide the interval</h4>

<ul>
  <li>
\[h = \frac{1-0}{2} = 0.5\]
  </li>
  <li>Points: \(x_0 = 0\), \(x_1 = 0.5\), \(x_2 = 1\)</li>
</ul>

<h4 id="step-2-evaluate-the-function">Step 2: Evaluate the function</h4>

<ul>
  <li>
\[f(0) = 0^2 = 0\]
  </li>
  <li>
\[f(0.5) = 0.25\]
  </li>
  <li>
\[f(1) = 1\]
  </li>
</ul>

<h4 id="step-3-apply-trapezoidal-formula">Step 3: Apply trapezoidal formula</h4>

\[I \approx \frac{0.5}{2} \left[ 0 + 2(0.25) + 1 \right]
= 0.25 \times (1.5)
= 0.375\]

<h4 id="exact-answer">Exact answer</h4>

<p>The exact value is:</p>

\[\int_0^1 x^2\,dx = \frac{1}{3} \approx 0.3333\]

<p>Thus, trapezoidal rule gives a slightly overestimated result.</p>

<hr />

<h3 id="example-2-simpsons-rule">Example 2: Simpson’s Rule</h3>

<p>Approximate:</p>

\[\int_0^1 x^2\,dx\]

<p>using \(n=2\) intervals.</p>

<h4 id="step-1-divide-the-interval-1">Step 1: Divide the interval</h4>

<ul>
  <li>
\[h = 0.5\]
  </li>
</ul>

<h4 id="step-2-evaluate-the-function-1">Step 2: Evaluate the function</h4>

<p>Already calculated above:</p>
<ul>
  <li>
\[f(0) = 0\]
  </li>
  <li>
\[f(0.5) = 0.25\]
  </li>
  <li>
\[f(1) = 1\]
  </li>
</ul>

<h4 id="step-3-apply-simpsons-formula">Step 3: Apply Simpson’s formula</h4>

\[I \approx \frac{0.5}{3} \left[ 0 + 4(0.25) + 1 \right]
= \frac{0.5}{3} \times (2)
= \frac{1}{3}
= 0.3333\]

<p>Thus, Simpson’s rule gives the exact value for polynomials of degree ≤ 3.</p>

<hr />

<h1 id="summary-table">Summary Table</h1>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Trapezoidal Rule</th>
      <th>Simpson’s Rule</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Approximation</td>
      <td>Straight line</td>
      <td>Parabola</td>
    </tr>
    <tr>
      <td>Accuracy</td>
      <td>\(O(h^2)\)</td>
      <td>\(O(h^4)\)</td>
    </tr>
    <tr>
      <td>Grid requirement</td>
      <td>Any number of intervals</td>
      <td>Even number of intervals</td>
    </tr>
    <tr>
      <td>When preferred</td>
      <td>Quick estimate, rough accuracy</td>
      <td>Higher precision with smooth functions</td>
    </tr>
  </tbody>
</table>

      </div>
      
    </article>
  </div>
</main>

        </div>

        <div class="search-content">
          <div class="inner">
  <label class="visually-hidden" for="search">Site Search</label>
        <input type="text" id="search" class="search-input" aria-describedby="results-count" tabindex="-1" placeholder="Enter your search term..." />
        <div id="results" class="results"></div>
</div>

        </div>
      </div>
    </div>

    <footer id="footer" class="site-footer">
  <div class="inner">
    <div class="copyright">
      
        <p>&copy; 2025 Rajesh Kumar.</p>
      
    </div>
  </div>
</footer>

    

<script async src="/SKMU/assets/javascripts/main.js"></script>

<script src="https://code.jquery.com/jquery-3.3.1.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
  <script src="http://localhost:4000/SKMU/assets/javascripts/lunr/lunr.min.js"></script>
  <script src="http://localhost:4000/SKMU/assets/javascripts/lunr/lunr.store.js"></script>
  <script>
    var idx = lunr(function () {
      
      // the, the normal lunr index initialization
      this.field('title')
      this.field('excerpt')
      this.field('categories')
      this.field('tags')
      this.ref('id')

      this.pipeline.remove(lunr.trimmer)

      // add documents to index
      for (var item in store) {
        this.add({
          title: store[item].title,
          excerpt: store[item].excerpt,
          categories: store[item].categories,
          tags: store[item].tags,
          id: item
        })
      }
    });

    $(document).ready(function () {
      $('input#search').on('keyup', function () {
        var resultdiv = $('#results');
        var query = $(this).val().toLowerCase();
        var result =
          idx.query(function (q) {
            query.split(lunr.tokenizer.separator).forEach(function (term) {
              q.term(term, { boost: 100 })
              if (query.lastIndexOf(" ") != query.length - 1) {
                q.term(term, { usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })
              }
              if (term != "") {
                q.term(term, { usePipeline: false, editDistance: 1, boost: 1 })
              }
            })
          });
        resultdiv.empty();
        resultdiv.prepend('<p id="results-count" class="results-found">' + result.length + ' Result(s) found</p>');
        for (var item in result) {
          var ref = result[item].ref;
          var searchitem =
            '<article class="entry">' +
              '<h3 class="entry-title">' +
                '<a href="' + store[ref].url + '">' + store[ref].title + '</a>' +
              '</h3>' +
              '<div class="entry-excerpt">' +
                '<p>' + store[ref].excerpt.split(" ").splice(0, 20).join(" ") + '...</p>' +
              '</div>' +
            '</article>';
          resultdiv.append(searchitem);
        }
      });
    });
  </script>



  </body>

</html>
